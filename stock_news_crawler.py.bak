#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import time
import datetime
import hashlib
import requests
import logging
from bs4 import BeautifulSoup
import schedule
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("stock_crawler.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("StockNewsCrawler")

# å…¨å±€å˜é‡
CONFIG_FILE = "config.json"
DATA_FILE = "news_data.json"
config = None
news_data = {}

# URLæ¨¡æ¿
URL_TEMPLATE = "https://guba.eastmoney.com/list,{code},1,f.html"

def should_filter_title(title):
    """åˆ¤æ–­æ ‡é¢˜æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
    # è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ ‡é¢˜
    filter_keywords = [
        # èµ„é‡‘ç›¸å…³
        "ä¸»åŠ›", "èµ„é‡‘", "æµå…¥", "æµå‡º", "å‡€ä¹°å…¥", "å‡€å–å‡º", "å¢æŒ", "å‡æŒ", 
        "ç­¹ç ", "è§£ç¦", "è§£å¥—", "å¥—ç‰¢", "è§£é”", "è§£å‹", "è§£é™¤é™å”®", 
        "æœºæ„", "åŸºé‡‘", "åŒ—å‘", "å—å‘", "å¤–èµ„", "æ¸¸èµ„", "æ•£æˆ·", "åº„å®¶",
        "å¤§å•", "å°å•", "å¤§å®—äº¤æ˜“", "æˆäº¤é‡", "æ¢æ‰‹ç‡",
        # è‚¡ä»·æ¶¨è·Œç›¸å…³çš„å…¶ä»–å…³é”®è¯
        "æ¶¨åœæ¿", "è·Œåœæ¿", "æ¶¨å¹…", "è·Œå¹…", "åå¼¹", "å›è°ƒ", "çªç ´", 
        "æ”¯æ’‘", "å‹åŠ›", "é˜»åŠ›", "çªç ´", "è·³ç©º", "ç¼ºå£", "ç›˜æ•´",
        "ç›˜ä¸­", "ç›˜å", "ç›˜é¢", "ç›˜å£", "ç›˜ä¸­å¼‚åŠ¨", "ç›˜åå¼‚åŠ¨"
    ]
    
    for keyword in filter_keywords:
        if keyword in title:
            return True
    
    return False

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    global config
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"é…ç½®åŠ è½½æˆåŠŸï¼Œç›‘æ§ {len(config['stocks'])} åªè‚¡ç¥¨")
        # è¾“å‡ºæ‰€æœ‰è¦ç›‘æ§çš„è‚¡ç¥¨å’ŒURL
        for stock in config['stocks']:
            url = URL_TEMPLATE.format(code=stock['code'])
            logger.info(f"å°†ç›‘æ§è‚¡ç¥¨: {stock['name']}({stock['code']}), URL: {url}")
        return True
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False

def load_news_data():
    """åŠ è½½å·²ä¿å­˜çš„æ–°é—»æ•°æ®"""
    global news_data
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                news_data = json.load(f)
            logger.info(f"å·²åŠ è½½ä¿å­˜çš„æ–°é—»æ•°æ®ï¼ŒåŒ…å« {len(news_data)} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"åŠ è½½æ–°é—»æ•°æ®å¤±è´¥: {e}")
            news_data = {}
    else:
        news_data = {}
        logger.info("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ–°é—»æ•°æ®ï¼Œå°†åˆ›å»ºæ–°çš„æ•°æ®æ–‡ä»¶")

def save_news_data():
    """ä¿å­˜æ–°é—»æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        logger.info(f"æ–°é—»æ•°æ®å·²ä¿å­˜ï¼Œå…± {len(news_data)} æ¡è®°å½•")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–°é—»æ•°æ®å¤±è´¥: {e}")

def generate_news_id(title, url):
    """ç”Ÿæˆæ–°é—»çš„å”¯ä¸€ID"""
    content = f"{title}_{url}".encode('utf-8')
    return hashlib.md5(content).hexdigest()

def clean_stock_price_info(title):
    """æ¸…é™¤æ ‡é¢˜ä¸­çš„è‚¡ä»·æ¶¨è·Œä¿¡æ¯"""
    # åŒ¹é…å¸¸è§çš„è‚¡ä»·æ¶¨è·Œæ¨¡å¼ï¼Œå¦‚ï¼šä¸Šæ¶¨5%ï¼Œä¸‹è·Œ3.2%ï¼Œ+2.5%ï¼Œ-1.8%ç­‰
    patterns = [
        r'ä¸Šæ¶¨\s*\d+(\.\d+)?%',
        r'ä¸‹è·Œ\s*\d+(\.\d+)?%',
        r'æ¶¨\s*\d+(\.\d+)?%',
        r'è·Œ\s*\d+(\.\d+)?%',
        r'[+-]\s*\d+(\.\d+)?%',
        r'å¤§æ¶¨\s*\d+(\.\d+)?%',
        r'å¤§è·Œ\s*\d+(\.\d+)?%',
        r'æ”¶ç›˜\s*[+-]?\s*\d+(\.\d+)?%',
        r'æŠ¥\s*[+-]?\s*\d+(\.\d+)?%',
        r'æ¶¨åœ',
        r'è·Œåœ'
    ]
    
    cleaned_title = title
    for pattern in patterns:
        cleaned_title = re.sub(pattern, '', cleaned_title)
    
    # å»é™¤å¤šä½™ç©ºæ ¼
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title).strip()
    return cleaned_title

def crawl_stock_news(stock):
    """æŠ“å–å•ä¸ªè‚¡ç¥¨çš„æœ€æ–°èµ„è®¯"""
    stock_name = stock['name']
    stock_code = stock['code']
    url = URL_TEMPLATE.format(code=stock_code)
    
    logger.info(f"å¼€å§‹æŠ“å– {stock_name}({stock_code}) çš„æœ€æ–°èµ„è®¯ï¼ŒURL: {url}")
    
    try:
        # ä½¿ç”¨MCP FireCrawlæŠ“å–é¡µé¢
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        logger.info(f"æ­£åœ¨è¯·æ±‚URL: {url}")
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        logger.info(f"è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è¾“å‡ºHTMLç»“æ„çš„å…³é”®éƒ¨åˆ†ï¼Œå¸®åŠ©åˆ†æ
        logger.debug("åˆ†æé¡µé¢HTMLç»“æ„...")
        
        # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶ï¼Œç”¨äºè°ƒè¯•
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(response.text)
        logger.debug("å·²ä¿å­˜é¡µé¢HTMLåˆ°debug_page.htmlæ–‡ä»¶")
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æŸ¥æ‰¾èµ„è®¯åˆ—è¡¨
        news_items = []
        
        # å°è¯•æ–¹æ³•1ï¼šæŸ¥æ‰¾è¡¨æ ¼ä¸­çš„è¡Œ
        table = soup.select_one('table.articleh')
        if table:
            logger.debug("æ‰¾åˆ°èµ„è®¯è¡¨æ ¼ table.articleh")
            rows = table.select('tr')
            if rows:
                news_items = rows
                logger.debug(f"ä»è¡¨æ ¼ä¸­æ‰¾åˆ° {len(rows)} è¡Œ")
        
        # å°è¯•æ–¹æ³•2ï¼šç›´æ¥æŸ¥æ‰¾articlehç±»çš„div
        if not news_items:
            items = soup.select('div.articleh')
            if items:
                news_items = items
                logger.debug(f"æ‰¾åˆ° {len(items)} ä¸ªdiv.articlehå…ƒç´ ")
        
        # å°è¯•æ–¹æ³•3ï¼šæŸ¥æ‰¾åŒ…å«èµ„è®¯çš„åˆ—è¡¨é¡¹
        if not news_items:
            items = soup.select('ul.newlist > li')
            if items:
                news_items = items
                logger.debug(f"æ‰¾åˆ° {len(items)} ä¸ªåˆ—è¡¨é¡¹")
        
        # å°è¯•æ–¹æ³•4ï¼šæŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„èµ„è®¯å®¹å™¨
        if not news_items:
            items = soup.select('.listcont .articleh, .articleh_list .articleh, #mainlist .articleh')
            if items:
                news_items = items
                logger.debug(f"ä»å¤šä¸ªå®¹å™¨ä¸­æ‰¾åˆ° {len(items)} ä¸ªèµ„è®¯é¡¹")
        
        # å°è¯•æ–¹æ³•5ï¼šæŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ ‡é¢˜å’Œé“¾æ¥çš„è¡Œ
        if not news_items:
            items = soup.select('tr:has(a), div:has(span.l3 > a)')
            if items:
                news_items = items
                logger.debug(f"æ‰¾åˆ° {len(items)} ä¸ªåŒ…å«é“¾æ¥çš„è¡Œ")
        
        if not news_items:
            logger.warning(f"æœªæ‰¾åˆ° {stock_name} çš„èµ„è®¯åˆ—è¡¨é¡¹ï¼ŒURL: {url}")
            return []
        
        logger.info(f"æ‰¾åˆ° {len(news_items)} æ¡èµ„è®¯é¡¹")
        
        # è¾“å‡ºå‰å‡ ä¸ªèµ„è®¯é¡¹çš„HTMLç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•
        for i, item in enumerate(news_items[:3]):
            logger.debug(f"èµ„è®¯é¡¹ {i+1} HTMLç»“æ„: {item}")
        
        news_list = []
        for idx, item in enumerate(news_items):
            try:
                # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = None
                
                # æ–¹æ³•1ï¼šæ ‡å‡†çš„ä¸œæ–¹è´¢å¯Œè‚¡å§æ ¼å¼
                title_tag = item.select_one('span.l3 > a')
                
                # æ–¹æ³•2ï¼šå…¶ä»–å¯èƒ½çš„æ ¼å¼
                if not title_tag:
                    title_tag = item.select_one('a.title, a.news_title, a[title]')
                
                # æ–¹æ³•3ï¼šä»»ä½•é“¾æ¥
                if not title_tag:
                    title_tag = item.select_one('a')
                
                if not title_tag:
                    logger.warning(f"ç¬¬ {idx+1} æ¡èµ„è®¯æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜æ ‡ç­¾")
                    continue
                
                title = title_tag.get_text().strip()
                news_url = title_tag.get('href')
                
                # å¦‚æœé“¾æ¥æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬ä¸ºç»å¯¹è·¯å¾„
                if news_url and not news_url.startswith('http'):
                    news_url = f"https://guba.eastmoney.com{news_url}"
                
                # è·³è¿‡æ²¡æœ‰URLçš„é¡¹ç›®
                if not news_url:
                    logger.debug(f"è·³è¿‡æ²¡æœ‰URLçš„èµ„è®¯: {title}")
                    continue
                
                # åªä¿ç•™ä»¥https://guba.eastmoney.com/newså¼€å¤´çš„èµ„è®¯
                if not news_url.startswith('https://guba.eastmoney.com/news'):
                    logger.debug(f"è·³è¿‡éèµ„è®¯URL: {news_url}")
                    continue
                
                # å°è¯•å¤šç§æ–¹å¼æå–å‘å¸ƒæ—¶é—´
                time_tag = None
                
                # æ–¹æ³•1ï¼šæ ‡å‡†çš„ä¸œæ–¹è´¢å¯Œè‚¡å§æ ¼å¼
                time_tag = item.select_one('span.l6')
                
                # æ–¹æ³•2ï¼šå…¶ä»–å¯èƒ½çš„æ ¼å¼
                if not time_tag:
                    time_tag = item.select_one('span.time, span.date, td:last-child')
                
                pub_time = time_tag.get_text().strip() if time_tag else ""
                
                # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹æå–
                if not pub_time:
                    # å°è¯•ä»URLæˆ–æ ‡é¢˜ä¸­æå–æ—¥æœŸ
                    date_match = re.search(r'(\d{2}-\d{2}|\d{2}/\d{2}|\d{4}-\d{2}-\d{2})', title)
                    if date_match:
                        pub_time = date_match.group(1)
                    else:
                        logger.debug(f"è·³è¿‡æ²¡æœ‰å‘å¸ƒæ—¶é—´çš„èµ„è®¯: {title}")
                        continue
                
                # å°è¯•å¤šç§æ–¹å¼æå–é˜…è¯»æ•°
                read_tag = item.select_one('span.l1')
                read_count = read_tag.get_text().strip() if read_tag else "0"
                
                # å°è¯•å¤šç§æ–¹å¼æå–è¯„è®ºæ•°
                comment_tag = item.select_one('span.l2')
                comment_count = comment_tag.get_text().strip() if comment_tag else "0"
                
                # å°è¯•å¤šç§æ–¹å¼æå–ä½œè€…ä¿¡æ¯
                author_tag = item.select_one('span.l4')
                author = author_tag.get_text().strip() if author_tag else ""
                
                # æ¸…é™¤æ ‡é¢˜ä¸­çš„è‚¡ä»·æ¶¨è·Œä¿¡æ¯
                cleaned_title = clean_stock_price_info(title)
                if cleaned_title != title:
                    logger.debug(f"æ¸…é™¤è‚¡ä»·ä¿¡æ¯: '{title}' -> '{cleaned_title}'")
                
                # è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ ‡é¢˜
                if should_filter_title(cleaned_title):
                    logger.debug(f"è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ ‡é¢˜: '{cleaned_title}'")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€ID
                news_id = generate_news_id(cleaned_title, news_url)
                
                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                news_list.append({
                    'id': news_id,
                    'title': cleaned_title,
                    'url': news_url,
                    'pub_time': pub_time,
                    'read_count': read_count,
                    'comment_count': comment_count,
                    'author': author,
                    'stock_name': stock_name,
                    'stock_code': stock_code,
                    'crawl_time': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
                
                logger.debug(f"æˆåŠŸè§£æèµ„è®¯: æ ‡é¢˜='{cleaned_title}', å‘å¸ƒæ—¶é—´={pub_time}, é˜…è¯»æ•°={read_count}, è¯„è®ºæ•°={comment_count}, ä½œè€…={author}")
                
            except Exception as e:
                logger.error(f"è§£æç¬¬ {idx+1} æ¡èµ„è®¯é¡¹æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"æˆåŠŸæŠ“å– {stock_name} çš„ {len(news_list)} æ¡æœ‰æ•ˆèµ„è®¯")
        return news_list
    
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ {url} å¤±è´¥: {e}")
        return []
    except Exception as e:
        logger.error(f"æŠ“å– {stock_name} èµ„è®¯å¤±è´¥: {e}")
        return []

def update_news_data(new_news_list):
    """æ›´æ–°æ–°é—»æ•°æ®ï¼Œé¿å…é‡å¤"""
    global news_data
    
    added_count = 0
    for news in new_news_list:
        news_id = news['id']
        if news_id not in news_data:
            news_data[news_id] = news
            added_count += 1
            logger.debug(f"æ–°å¢èµ„è®¯: {news['title']} ({news['stock_name']})")
    
    logger.info(f"æ–°å¢ {added_count} æ¡æ–°é—»")
    
    # æŒ‰è‚¡ç¥¨ä»£ç ä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
    save_news_by_stock()
    
    return added_count

def save_news_by_stock():
    """æŒ‰è‚¡ç¥¨ä»£ç å°†æ–°é—»ä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶"""
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = "stock_data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        logger.info(f"åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}")
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„æ•´ç†æ–°é—»
    stock_news = {}
    for news_id, news in news_data.items():
        # åªä¿å­˜ä»¥https://guba.eastmoney.com/newså¼€å¤´çš„èµ„è®¯
        if not news['url'].startswith('https://guba.eastmoney.com/news'):
            logger.debug(f"è·³è¿‡éèµ„è®¯URL: {news['url']}")
            continue
        
        # è¿‡æ»¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ ‡é¢˜
        if should_filter_title(news['title']):
            logger.debug(f"è·³è¿‡åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ ‡é¢˜: {news['title']}")
            continue
            
        stock_code = news['stock_code']
        if stock_code not in stock_news:
            stock_news[stock_code] = []
        stock_news[stock_code].append(news)
    
    # å¯¹æ¯ä¸ªè‚¡ç¥¨çš„æ–°é—»æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    for stock_code in stock_news:
        stock_news[stock_code].sort(key=lambda x: x['pub_time'], reverse=True)
        
        # ä¿å­˜åˆ°å¯¹åº”çš„æ–‡ä»¶
        file_path = os.path.join(data_dir, f"{stock_code}.json")
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(stock_news[stock_code], f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {stock_code} çš„ {len(stock_news[stock_code])} æ¡èµ„è®¯åˆ° {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜ {stock_code} çš„èµ„è®¯åˆ°æ–‡ä»¶å¤±è´¥: {e}")

def generate_html():
    """ç”ŸæˆHTMLé¡µé¢"""
    output_file = config.get('output_file', 'stock_news.html')
    logger.info(f"å¼€å§‹ç”ŸæˆHTMLé¡µé¢: {output_file}")
    
    # ä»æ–‡ä»¶è¯»å–å„è‚¡ç¥¨çš„æ–°é—»æ•°æ®
    stock_news = {}
    data_dir = "stock_data"
    
    if not os.path.exists(data_dir):
        logger.warning(f"æ•°æ®ç›®å½• {data_dir} ä¸å­˜åœ¨ï¼Œæ— æ³•ç”ŸæˆHTML")
        return
    
    # è¯»å–æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®æ–‡ä»¶
    for stock in config['stocks']:
        stock_code = stock['code']
        file_path = os.path.join(data_dir, f"{stock_code}.json")
        
        if not os.path.exists(file_path):
            logger.warning(f"è‚¡ç¥¨ {stock_code} çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                stock_news[stock_code] = json.load(f)
            
            # ç¡®ä¿æŒ‰å‘å¸ƒæ—¶é—´é€†åºæ’åº
            stock_news[stock_code].sort(key=lambda x: x['pub_time'], reverse=True)
            logger.info(f"å·²è¯»å– {stock_code} çš„ {len(stock_news[stock_code])} æ¡èµ„è®¯")
        except Exception as e:
            logger.error(f"è¯»å– {stock_code} çš„èµ„è®¯æ–‡ä»¶å¤±è´¥: {e}")
    
    # ç”ŸæˆHTMLå†…å®¹
    html_content = """
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>è‚¡ç¥¨æœ€æ–°èµ„è®¯</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .container {
                max-width: 1200px;
                margin: 0 auto;
            }
            .header {
                background-color: #2c3e50;
                color: white;
                padding: 20px;
                border-radius: 5px 5px 0 0;
                margin-bottom: 20px;
            }
            .stock-section {
                background-color: white;
                border-radius: 5px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.1);
                margin-bottom: 20px;
                overflow: hidden;
            }
            .stock-header {
                background-color: #3498db;
                color: white;
                padding: 10px 20px;
                display: flex;
                justify-content: space-between;
                align-items: center;
            }
            .news-list {
                padding: 0;
                margin: 0;
                list-style: none;
            }
            .news-item {
                padding: 15px 20px;
                border-bottom: 1px solid #eee;
            }
            .news-item:last-child {
                border-bottom: none;
            }
            .news-title {
                margin: 0 0 5px 0;
                font-size: 16px;
            }
            .news-title a {
                color: #2c3e50;
                text-decoration: none;
            }
            .news-title a:hover {
                text-decoration: underline;
            }
            .news-meta {
                color: #7f8c8d;
                font-size: 12px;
            }
            .stats {
                display: inline-block;
                margin-right: 15px;
            }
            .stats i {
                margin-right: 5px;
            }
            .footer {
                text-align: center;
                margin-top: 20px;
                color: #7f8c8d;
                font-size: 12px;
            }
            .update-time {
                text-align: right;
                color: #7f8c8d;
                font-size: 12px;
                padding: 10px 20px;
            }
            .pagination {
                display: flex;
                justify-content: center;
                margin: 20px 0;
                padding: 0;
                list-style: none;
            }
            .pagination li {
                margin: 0 5px;
            }
            .pagination a {
                display: block;
                padding: 8px 12px;
                background-color: #3498db;
                color: white;
                text-decoration: none;
                border-radius: 3px;
            }
            .pagination a:hover {
                background-color: #2980b9;
            }
            .pagination .active a {
                background-color: #2c3e50;
            }
            .pagination .disabled a {
                background-color: #bdc3c7;
                cursor: not-allowed;
            }
        </style>
        <script>
            function showPage(stockCode, pageNum) {
                // éšè—æ‰€æœ‰é¡µé¢
                var pages = document.querySelectorAll('.page-' + stockCode);
                for (var i = 0; i < pages.length; i++) {
                    pages[i].style.display = 'none';
                }
                
                // æ˜¾ç¤ºé€‰ä¸­çš„é¡µé¢
                var selectedPage = document.getElementById('page-' + stockCode + '-' + pageNum);
                if (selectedPage) {
                    selectedPage.style.display = 'block';
                }
                
                // æ›´æ–°åˆ†é¡µæŒ‰é’®çŠ¶æ€
                var paginationLinks = document.querySelectorAll('.pagination-' + stockCode + ' li');
                for (var i = 0; i < paginationLinks.length; i++) {
                    paginationLinks[i].classList.remove('active');
                }
                
                var activeLink = document.querySelector('.pagination-' + stockCode + ' li[data-page="' + pageNum + '"]');
                if (activeLink) {
                    activeLink.classList.add('active');
                }
                
                return false;
            }
        </script>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>è‚¡ç¥¨æœ€æ–°èµ„è®¯</h1>
                <p class="update-time">æœ€åæ›´æ–°æ—¶é—´: """ + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """</p>
            </div>
    """
    
    # æ¯é¡µæ˜¾ç¤ºçš„æ–°é—»æ•°é‡ - ä»é…ç½®ä¸­è·å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º10
    items_per_page = config.get('max_news_per_stock', 10)
    
    # æ·»åŠ æ¯åªè‚¡ç¥¨çš„æ–°é—»
    for stock in config['stocks']:
        stock_code = stock['code']
        stock_name = stock['name']
        
        if stock_code not in stock_news or not stock_news[stock_code]:
            logger.info(f"è‚¡ç¥¨ {stock_name}({stock_code}) æ²¡æœ‰æ–°é—»ï¼Œè·³è¿‡")
            continue
        
        news_list = stock_news[stock_code]
        total_news = len(news_list)
        total_pages = (total_news + items_per_page - 1) // items_per_page  # å‘ä¸Šå–æ•´
        
        html_content += f"""
            <div class="stock-section">
                <div class="stock-header">
                    <h2>{stock_name} ({stock_code})</h2>
                    <span>å…± {total_news} æ¡èµ„è®¯</span>
                </div>
        """
        
        # åˆ†é¡µæ˜¾ç¤ºæ–°é—»
        for page in range(1, total_pages + 1):
            start_idx = (page - 1) * items_per_page
            end_idx = min(start_idx + items_per_page, total_news)
            page_news = news_list[start_idx:end_idx]
            
            display_style = "block" if page == 1 else "none"
            
            html_content += f"""
                <div id="page-{stock_code}-{page}" class="page-{stock_code}" style="display: {display_style}">
                    <ul class="news-list">
            """
            
            for news in page_news:
                html_content += f"""
                        <li class="news-item">
                            <h3 class="news-title"><a href="{news['url']}" target="_blank">{news['title']}</a></h3>
                            <div class="news-meta">
                                <span class="stats">ğŸ‘ï¸ é˜…è¯»: {news.get('read_count', '0')}</span>
                                <span class="stats">ğŸ’¬ è¯„è®º: {news.get('comment_count', '0')}</span>
                                <span class="stats">ğŸ‘¤ ä½œè€…: {news.get('author', 'æœªçŸ¥')}</span>
                                <span class="stats">â±ï¸ å‘å¸ƒæ—¶é—´: {news['pub_time']}</span>
                                <span class="stats">ğŸ”„ æŠ“å–æ—¶é—´: {news['crawl_time']}</span>
                            </div>
                        </li>
                """
            
            html_content += """
                    </ul>
                </div>
            """
        
        # æ·»åŠ åˆ†é¡µå¯¼èˆª
        if total_pages > 1:
            html_content += f"""
                <div class="pagination-container" style="padding: 10px 20px;">
                    <ul class="pagination pagination-{stock_code}">
            """
            
            for page in range(1, total_pages + 1):
                active_class = "active" if page == 1 else ""
                html_content += f"""
                        <li class="{active_class}" data-page="{page}"><a href="javascript:void(0)" onclick="showPage('{stock_code}', {page}); return false;">{page}</a></li>
                """
            
            html_content += """
                    </ul>
                </div>
            """
        
        html_content += """
            </div>
        """
    
    html_content += """
            <div class="footer">
                <p>Â© """ + datetime.datetime.now().strftime('%Y') + """ è‚¡ç¥¨èµ„è®¯çˆ¬è™« | æ•°æ®æ¥æº: ä¸œæ–¹è´¢å¯Œç½‘</p>
            </div>
        </div>
    </body>
    </html>
    """
    
    # å†™å…¥æ–‡ä»¶
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"HTMLé¡µé¢å·²ç”Ÿæˆ: {output_file}")
    except Exception as e:
        logger.error(f"ç”ŸæˆHTMLé¡µé¢å¤±è´¥: {e}")

def crawl_all_stocks():
    """æŠ“å–æ‰€æœ‰è‚¡ç¥¨çš„èµ„è®¯"""
    if not config:
        logger.error("é…ç½®æœªåŠ è½½ï¼Œæ— æ³•æŠ“å–")
        return
    
    logger.info("å¼€å§‹æŠ“å–æ‰€æœ‰è‚¡ç¥¨çš„èµ„è®¯")
    all_news = []
    for stock in config['stocks']:
        logger.info(f"å‡†å¤‡æŠ“å– {stock['name']}({stock['code']}) çš„èµ„è®¯")
        news_list = crawl_stock_news(stock)
        all_news.extend(news_list)
        # æ·»åŠ å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        delay_seconds = 2
        logger.info(f"ç­‰å¾… {delay_seconds} ç§’åç»§ç»­ä¸‹ä¸€åªè‚¡ç¥¨")
        time.sleep(delay_seconds)
    
    # æ›´æ–°æ•°æ®
    added_count = update_news_data(all_news)
    
    # å¦‚æœæœ‰æ–°å¢æ•°æ®ï¼Œä¿å­˜æ•°æ®å¹¶æ›´æ–°HTML
    if added_count > 0:
        logger.info(f"æœ‰ {added_count} æ¡æ–°èµ„è®¯ï¼Œæ›´æ–°æ•°æ®å’ŒHTML")
        save_news_data()
        generate_html()
    else:
        logger.info("æ²¡æœ‰æ–°èµ„è®¯ï¼Œè·³è¿‡æ›´æ–°")
    
    logger.info(f"æœ¬æ¬¡æŠ“å–å®Œæˆï¼Œæ–°å¢ {added_count} æ¡èµ„è®¯")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("è‚¡ç¥¨èµ„è®¯çˆ¬è™«å¯åŠ¨")
    
    # åŠ è½½é…ç½®
    if not load_config():
        logger.error("é…ç½®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # åŠ è½½å·²ä¿å­˜çš„æ–°é—»æ•°æ®
    load_news_data()
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡æŠ“å–
    logger.info("å¼€å§‹ç¬¬ä¸€æ¬¡æŠ“å–")
    crawl_all_stocks()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡
    interval_minutes = config.get('update_interval_minutes', 60)
    logger.info(f"è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯ {interval_minutes} åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡")
    
    schedule.every(interval_minutes).minutes.do(crawl_all_stocks)
    
    # è¿è¡Œå®šæ—¶ä»»åŠ¡
    logger.info("è¿›å…¥å®šæ—¶ä»»åŠ¡å¾ªç¯")
    while True:
        schedule.run_pending()
        time.sleep(1)

if __name__ == "__main__":
    main() 